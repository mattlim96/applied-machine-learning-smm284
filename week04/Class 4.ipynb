{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5315d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "\n",
    "# standard numerical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# to process text\n",
    "import re \n",
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import operator\n",
    "import datetime\n",
    "import unidecode\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import os\n",
    "\n",
    "# to carry out topic modeling\n",
    "import gensim\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import ldamodel as lda\n",
    "\n",
    "# to carry out sentiment analysis\n",
    "nltk.download(\"vader_lexicon\")\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346c316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to install a package you don't have installed (remove the hashtags below to run)\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset (available from Moodle, need to load the data into Google Colab)\n",
    "\n",
    "data = pd.read_csv(\"Class4.csv\", sep=';').set_index('id')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b638f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the data\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eed684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change abstract data to a list as easier to work with\n",
    "\n",
    "data1 = data['text'].values.tolist()\n",
    "\n",
    "# Note 'pprint' is just a more visually appealing version of print - standing for 'pretty print'\n",
    "\n",
    "pprint(data1[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now a big block of basic text pre-processing\n",
    "# this code will work with any text, and is a recommended starting point for text pre-processing\n",
    "\n",
    "# I've included a timer here, to get an idea of how long it takes to process\n",
    "# Note: this timer has an end-line at end of the code\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "data2 = []\n",
    "\n",
    "for tweet in range(0, len(data1)):\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    p_data = re.sub(r'\\W', ' ', str(data1[tweet]))\n",
    "    p_data = unidecode.unidecode(p_data)\n",
    "\n",
    "    # remove all single characters\n",
    "    p_data = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', p_data)\n",
    "    p_data = re.sub(r'\\^[a-zA-Z]\\s+', ' ', p_data) \n",
    "    \n",
    "    # remove all numbers\n",
    "    p_data = re.sub(r'\\d+','', p_data) \n",
    "\n",
    "    # substitute multiple white spaces with single space\n",
    "    p_data = re.sub(r'\\s+', ' ', p_data, flags=re.I)\n",
    "\n",
    "    # Remove prefixed 'b'\n",
    "    p_data = re.sub(r'^b\\s+', '', p_data)\n",
    "    \n",
    "\n",
    "    data2.append(p_data)\n",
    "    \n",
    "    \n",
    "print('Basic pre-processing of dataset took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "pprint(data2[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debfd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing: Change tweets to bag-of-words\n",
    "\n",
    "def sent_to_words(tweets):\n",
    "    for tweet in tweets:\n",
    "        yield(gensim.utils.simple_preprocess(str(tweet), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data2))\n",
    "\n",
    "count = sum([len(tweet) for tweet in data_words])\n",
    "print('Total number of terms across all tweets is: ', count)\n",
    "\n",
    "print(data_words[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59eabe7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english') # nltk stopwords - about 180 stopwords\n",
    "\n",
    "stop_words.extend(['rt', 'gt', 'group', 'june', 'years', 'right', 'another', 'emini', 'say', 'gnus',\n",
    "                  'join', 'link', 'nq', 'agnes', 'per', 'lakshmi_']) # personal stopwords based on text inspection\n",
    "\n",
    "# define a function to remove stopwords\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] \n",
    "            for doc in texts]\n",
    "\n",
    "# remove stopwords\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "print('Remove stopwords took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "count = sum([len(tweet) for tweet in data_words_nostops])\n",
    "print('Total number of terms across all tweets is: ', count)\n",
    "\n",
    "print(data_words_nostops[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b8d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create a document-term-matrix\n",
    "\n",
    "# Create Dictionary\n",
    "data_dict = Dictionary(data_words_nostops)\n",
    "print(\"Length of initial dictionary is: \", (len(data_dict)))\n",
    "\n",
    "data_dict.filter_extremes(no_below=5, no_above=0.2)\n",
    "print(\"Length of reduced dictionary is: \", (len(data_dict)))\n",
    "\n",
    "data_dict.filter_n_most_frequent(100)  \n",
    "print(\"Length of reduced dictionary is: \", (len(data_dict)))\n",
    "\n",
    "# Create Corpus\n",
    "data_texts = data_words_nostops.copy()\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [data_dict.doc2bow(text) for text in data_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find low term documents and remove (tweets must have at least two terms)\n",
    "# We want to do this as we want to relate tweets to each other, so need enough terms\n",
    "\n",
    "# Convert corpus to a numpy matrix\n",
    "numpy_matrix_eg = gensim.matutils.corpus2dense(corpus, num_terms=2212) # adjust based on length of dictionary\n",
    "\n",
    "# Print word count per document\n",
    "column_sums = [sum([row[i] for row in numpy_matrix_eg]) for i in range(0,len(numpy_matrix_eg[0]))]\n",
    "counter=collections.Counter(column_sums)\n",
    "od = collections.OrderedDict(sorted(counter.items()))\n",
    "\n",
    "# Create a filtered matrix that removes docs with less than 2 terms\n",
    "n_matrix_eg2 = numpy_matrix_eg[:, (numpy_matrix_eg != 0).sum(axis=0) > 1]\n",
    "\n",
    "# Confirm that matrix has removed the docs\n",
    "column_sums2 = [sum([row[i] for row in n_matrix_eg2]) for i in range(0,len(n_matrix_eg2[0]))]\n",
    "counter2=collections.Counter(column_sums2)\n",
    "od2 = collections.OrderedDict(sorted(counter2.items()))\n",
    "\n",
    "# Convert matrix back to a corpus\n",
    "corpus_eg = gensim.matutils.Dense2Corpus(n_matrix_eg2)\n",
    "\n",
    "# Confirm change has been made\n",
    "print(len(corpus)) # original corpus\n",
    "print(len(corpus_eg)) # new corpus\n",
    "\n",
    "# you don't want to lose more than about 10% of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c22e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic LDA test model\n",
    "\n",
    "# key choice is the number of topics that best describes the documents, which is a 'guess'\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lda10 = lda.LdaModel(corpus_eg, num_topics=10, id2word = data_dict, \n",
    "                     random_state=20, eval_every=None)\n",
    "\n",
    "\n",
    "print('Run single LDA model took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "# Show Topics\n",
    "pprint(lda10.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "# a statistical estimate of whether a human reader would consider the topics to be 'coherent'\n",
    "# varies between 0 (no coherence) and 1 (fully coherent)\n",
    "coherence_model_lda10 = CoherenceModel(model=lda10, texts=data_texts, \n",
    "                                            dictionary=data_dict, coherence='c_v')\n",
    "coherence_lda10 = coherence_model_lda10.get_coherence()\n",
    "\n",
    "print('Calculate coherence score took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "print('\\nCoherence Score: ', coherence_lda10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ff7f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate multiple coherence scores took 0:08:12.463445\n",
      "[{'alpha': 1,\n",
      "  'coherence': 0.5957766550086534,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 20},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5929419545833838,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5799671781774622,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.579236604533144,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5783097427524253,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 10},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5777347382355575,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.575042502052432,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 10},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5729505828084405,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5727892239830569,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5727892239830569,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 20},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5715363231324886,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5695778691687678,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5688772482591297,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 15},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5679490339998344,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5668878721107957,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5656293769563412,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5650360250710004,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5619999240545779,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5607351478081679,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5596670130190976,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5594238600367936,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 20},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.558840411575519,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5585705485233728,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5583007545556472,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5581859171868678,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5581221659623227,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 10},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5579759338171036,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5579227977138855,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5566154813968968,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 15},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5565877992001723,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5560739036096962,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 15},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5556316985609965,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5551631028916145,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5550976130046871,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 20},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5546770309753737,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5545734415044301,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5536185467792882,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5524580096830737,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 20},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5523745193665943,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 20},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5523709777552347,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5521445244383364,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5518759585327798,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 20},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5509179598421419,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5509062088797342,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5505789808641873,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 10},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5499974193781827,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5493467987034135,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5491162227042908,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5487770910461832,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 10},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5486767981893246,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5482994571586792,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 15},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5481692989409663,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 20},\n",
      " {'alpha': 10,\n",
      "  'coherence': 0.5476602095599842,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5472480355199202,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5468822152545132,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5464325102339033,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 10},\n",
      " {'alpha': 5,\n",
      "  'coherence': 0.5463763939506691,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 20},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5457240910724533,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 20},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5455285388331368,\n",
      "  'num_topics': 15,\n",
      "  'random_state': 15},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5446157411423207,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 10},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5429847101182691,\n",
      "  'num_topics': 18,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5401131724243085,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5349326668637092,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5337686922468619,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5334378216964683,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.5317643277154119,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 20},\n",
      " {'alpha': 0.1,\n",
      "  'coherence': 0.527815627869888,\n",
      "  'num_topics': 21,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5264827313594033,\n",
      "  'num_topics': 12,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5233409148811452,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 10},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5229899121361893,\n",
      "  'num_topics': 9,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.5146980781625742,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 15},\n",
      " {'alpha': 1,\n",
      "  'coherence': 0.512034497286142,\n",
      "  'num_topics': 6,\n",
      "  'random_state': 10}]\n"
     ]
    }
   ],
   "source": [
    "# Finding the best LDA Model\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, id2word, limit, start=2, step=2):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Dictionary\n",
    "    id2word : Dictionary\n",
    "    corpus : Corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number \n",
    "    of topics\n",
    "    \"\"\"\n",
    "    alpha = [0.1, 1, 5, 10]\n",
    "    random_state = [10, 15, 20]\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    output_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        for a in alpha:\n",
    "            for r in random_state:\n",
    "                model = lda.LdaModel(corpus=corpus, num_topics=num_topics, id2word=id2word, \n",
    "                    random_state=r, alpha=a, eval_every=None)\n",
    "                model_list.append(model)\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, \n",
    "                                                coherence='c_v')\n",
    "                coherence_values.append(coherencemodel.get_coherence())\n",
    "                model_output ={'alpha': a, 'random_state': r, 'coherence': \n",
    "                               coherencemodel.get_coherence(), 'num_topics': num_topics}\n",
    "                output_list.append(model_output)\n",
    "\n",
    "    return model_list, coherence_values, output_list\n",
    "\n",
    "model_list, coherence_values, output_list = compute_coherence_values(dictionary=data_dict, \n",
    "                                                                     corpus=corpus_eg, \n",
    "                                                                     texts=data_texts,\n",
    "                                                                     id2word=data_dict,\n",
    "                                                                     start=6, limit=24, step=3)\n",
    "\n",
    "print('Calculate multiple coherence scores took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "output_list_descending = sorted(output_list, key=lambda d: d['coherence'], reverse=True)\n",
    "pprint(output_list_descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best LDA model\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "lda21 = lda.LdaModel(corpus_eg, num_topics=21, alpha=1, id2word = data_dict, \n",
    "                     random_state=20, eval_every=None)\n",
    "\n",
    "\n",
    "print('Run best LDA model took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "# Show Topics\n",
    "pprint(lda21.show_topics(formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "# a statistical estimate of whether a human reader would consider the topics to be 'coherent'\n",
    "# varies between 0 (no coherence) and 1 (fully coherent)\n",
    "coherence_model_lda21 = CoherenceModel(model=lda21, texts=data_texts, \n",
    "                                            dictionary=data_dict, coherence='c_v')\n",
    "coherence_lda21 = coherence_model_lda21.get_coherence()\n",
    "\n",
    "print('Calculate coherence score took %s' % str(datetime.datetime.now() - start))\n",
    "\n",
    "print('\\nCoherence Score: ', coherence_lda21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "sent_analyzer = SentimentIntensityAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986be58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the analyser\n",
    "\n",
    "bayes = \"ugh. This is too much machine learning in a single day. Please make it stop!!!\"\n",
    "print(sent_analyzer.polarity_scores(bayes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn our list of cleaned tweets back into a dataframe\n",
    "\n",
    "sent_df = pd.DataFrame(data2)\n",
    "sent_df.columns =['tweet_text']\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7dcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply sentiment to the dataset\n",
    "def polarity(data):\n",
    "  polarity = \"neutral\"\n",
    "  if(data['compound']>= 0.05):\n",
    "    polarity = \"positive\"\n",
    "  elif(data['compound']<= -0.05):\n",
    "    polarity = \"negative\"\n",
    "  return polarity\n",
    "\n",
    "\n",
    "def predict_sentiment(text):\n",
    "  data =  sent_analyzer.polarity_scores(text)\n",
    "  return polarity(data)\n",
    "\n",
    "\n",
    "# Run the predictions\n",
    "sent_df[\"sent_prediction\"] = sent_df[\"tweet_text\"].apply(predict_sentiment)\n",
    "\n",
    "sent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480abd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['sent_prediction'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
